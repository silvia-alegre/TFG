
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% SAAC %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{arasaac,
    author       = {{ARASAAC}},
    title        = {ARASAAC - Portal de Comunicación Aumentativa y Alternativa},
    year         = 2024,
    url          = {https://arasaac.org/aac/es},
    note         = {Accessed: 2024-08-04}
}

@misc{asterics_grid_comunicador,
    author       = {{Aula Abierta ARASAAC}},
    title        = {Asterics Grid: Pantalla de Inicio del Comunicador},
    year         = 2024,
    url          = {https://aulaabierta.arasaac.org/asterics-grid_pantalla-de-inicio-del-comunicador},
    note         = {Accessed: 2024-08-04}
}

@book{murphy2022probabilistic,
  title={Probabilistic Machine Learning: An Introduction},
  author={Murphy, Kevin P.},
  year={2022},
  publisher={The MIT Press},
  address={Cambridge, Massachusetts; London, England}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% APRENDIZAJE AUTOMÁTICO %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@Inbook{samuelCheckers,
  editor = {Sammut, Claude and Webb, Geoffrey I.},
  title = {Samuel's Checkers Player},
  bookTitle = {Encyclopedia of Machine Learning},
  year = {2010},
  publisher = {Springer US},
  address = {Boston, MA},
  pages = {881--881},
  isbn = {978-0-387-30164-8},
  url = {\url{https://doi.org/10.1007/978-0-387-30164-8_740}},
}


@book{mitchell1997mcgraw,
  title={Machine Learningl},
  author={Mitchell, Tom},
  publisher={McGraw-Hil},
  pages={154--200},
  year={1997}
}

@book{mirtaheri2022machine,
  title={Machine Learning},
  author={Mirtaheri, Seyedeh Leili and Shahbazian, Reza},
  year={2022},
  publisher={CRC Press},
}

@book{alma997066713303706,
author = {Sowmya V. B. and Majumder, Bodhisattwa and Gupta, Anuj and Surana, Harshit},
address = {Sebastopol, CA},
booktitle = {Practical natural language processing : a comprehensive guide to building real-world NLP systems},
edition = {First edition.},
isbn = {1-4920-5402-X},
keywords = {Application software -- Development},
language = {eng},
publisher = {O'Reilly Media},
title = {Practical natural language processing : a comprehensive guide to building real-world NLP systems },
year = {2020 - 2020},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% REDES NEURONALES %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={Bulletin of Mathematical Biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer},
  doi={10.1007/BF02478259}
}

@book{jurafsky2023speech,
  title={Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author={Jurafsky, Daniel and Martin, James H.},
  edition={Third Edition draft},
  publisher={Stanford University and University of Colorado at Boulder},
  year={2023},
  note={Draft version}
}

@misc{sutskever2014sequencesequencelearningneural,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.3215}, 
}
@Inbook{Rojas1996,
author="Rojas, Ra{\'u}l",
title="The Backpropagation Algorithm",
bookTitle="Neural Networks: A Systematic Introduction",
year="1996",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="149--182",
abstract="We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems---the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.",
isbn="978-3-642-61068-4",
doi="10.1007/978-3-642-61068-4_7",
url="https://doi.org/10.1007/978-3-642-61068-4_7"
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% TRANSFORMERS %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@INPROCEEDINGS{multiheaddotproduct,
  author={Bilonoh, Bohdan and Mashtalir, Sergii},
  booktitle={2020 IEEE Third International Conference on Data Stream Mining \& Processing (DSMP)}, 
  title={Parallel multi-head dot product attention for video summarization}, 
  year={2020},
  volume={},
  number={},
  pages={158-162},
  keywords={Computational modeling;Computer architecture;Training;YouTube;Natural language processing;Adaptation models;Task analysis;video summarization;sequence to sequence;self-attention;Transformer},
  doi={10.1109/DSMP47368.2020.9204059}}

@misc{cordonnier2021multiheadattentioncollaborateinstead,
      title={Multi-Head Attention: Collaborate Instead of Concatenate}, 
      author={Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},
      year={2021},
      eprint={2006.16362},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16362}, 
}

@misc{ba2016layernormalization,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1607.06450}, 
}

@misc{sriram2017coldfusiontrainingseq2seq,
      title={Cold Fusion: Training Seq2Seq Models Together with Language Models}, 
      author={Anuroop Sriram and Heewoo Jun and Sanjeev Satheesh and Adam Coates},
      year={2017},
      eprint={1708.06426},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1708.06426}, 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% LANGUAGE MODELS %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{burtsev2023working,
  title={The Working Limitations of Large Language Models},
  author={Burtsev, Mikhail and Reeves, Martin and Job, Adam},
  journal={MIT Sloan Management Review},
  year={2023},
  month={November},
  pages={7},
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{dai2019transformerxlattentivelanguagemodels,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.02860}, 
}

@misc{gu2017trainablegreedydecodingneural,
      title={Trainable Greedy Decoding for Neural Machine Translation}, 
      author={Jiatao Gu and Kyunghyun Cho and Victor O. K. Li},
      year={2017},
      eprint={1702.02429},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1702.02429}, 
}

@misc{nadeem2020systematiccharacterizationsamplingalgorithms,
      title={A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation}, 
      author={Moin Nadeem and Tianxing He and Kyunghyun Cho and James Glass},
      year={2020},
      eprint={2009.07243},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.07243}, 
}

@misc{holtzman2020curiouscaseneuraltext,
      title={The Curious Case of Neural Text Degeneration}, 
      author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
      year={2020},
      eprint={1904.09751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09751}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI (2023)},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{gemmateam2024gemmaopenmodelsbased,
      title={Gemma: Open Models Based on Gemini Research and Technology}, 
      author={Gemma Team},
      year={2024},
      eprint={2403.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08295}, 
}

@misc{geminiteam2024geminifamilyhighlycapable,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}
@misc{christiano2023deepreinforcementlearninghuman,
      title={Deep reinforcement learning from human preferences}, 
      author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
      year={2023},
      eprint={1706.03741},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1706.03741}, 
}

@misc{shazeer2019fasttransformerdecodingwritehead,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1911.02150}, 
}

@misc{su2023roformerenhancedtransformerrotary,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202}, 
}

@misc{zhang2019rootmeansquarelayer,
      title={Root Mean Square Layer Normalization}, 
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.07467}, 
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{liu2018generatingwikipediasummarizinglong,
      title={Generating Wikipedia by Summarizing Long Sequences}, 
      author={Peter J. Liu and Mohammad Saleh and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
      year={2018},
      eprint={1801.10198},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1801.10198}, 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Adaptación LLM %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@ARTICLE{9134370,
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE}, 
  title={A Comprehensive Survey on Transfer Learning}, 
  year={2021},
  volume={109},
  number={1},
  pages={43-76},
  keywords={Transfer learning;Semisupervised learning;Data models;Covariance matrices;Machine learning;Adaptation models;Domain adaptation;interpretation;machine learning;transfer learning},
  doi={10.1109/JPROC.2020.3004555}}

@article{Weiss2016,
  author    = {Karl Weiss and Taghi M. Khoshgoftaar and DingDing Wang},
  title     = {A survey of transfer learning},
  journal   = {Journal of Big Data},
  year      = {2016},
  volume    = {3},
  number    = {1},
  pages     = {9},
  doi       = {10.1186/s40537-016-0043-6},
  url       = {https://doi.org/10.1186/s40537-016-0043-6},
  issn      = {2196-1115}
}

@INPROCEEDINGS{8659197,
  author={Wang, Tianyang and Huan, Jun and Zhu, Michelle},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Instance-Based Deep Transfer Learning}, 
  year={2019},
  volume={},
  number={},
  pages={367-375},
  keywords={Training;Data models;Training data;Computational modeling;Deep learning;Optimization;Load modeling},
  doi={10.1109/WACV.2019.00045}}

@misc{he2021effectivenessadapterbasedtuningpretrained,
      title={On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation}, 
      author={Ruidan He and Linlin Liu and Hai Ye and Qingyu Tan and Bosheng Ding and Liying Cheng and Jia-Wei Low and Lidong Bing and Luo Si},
      year={2021},
      eprint={2106.03164},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.03164}, 
}

@misc{houlsby2019parameterefficienttransferlearningnlp,
      title={Parameter-Efficient Transfer Learning for NLP}, 
      author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
      year={2019},
      eprint={1902.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.00751}, 
}

@misc{stickland2019bertpalsprojectedattention,
      title={BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning}, 
      author={Asa Cooper Stickland and Iain Murray},
      year={2019},
      eprint={1902.02671},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.02671}, 
}

%peft
@misc{hu2023llmadaptersadapterfamilyparameterefficient,
      title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models}, 
      author={Zhiqiang Hu and Lei Wang and Yihuai Lan and Wanyu Xu and Ee-Peng Lim and Lidong Bing and Xing Xu and Soujanya Poria and Roy Ka-Wei Lee},
      year={2023},
      eprint={2304.01933},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01933}, 
}

%lora
@article{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@inproceedings{choukroun2019low,
  title={Low-bit quantization of neural networks for efficient inference},
  author={Choukroun, Yoni and Kravchik, Eli and Yang, Fan and Kisilev, Pavel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  pages={3009--3018},
  year={2019},
  organization={IEEE}
}

@misc{micikevicius2018mixedprecisiontraining,
      title={Mixed Precision Training}, 
      author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
      year={2018},
      eprint={1710.03740},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1710.03740}, 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% cuantización %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{xiao2024smoothquantaccurateefficientposttraining,
      title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}, 
      author={Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han},
      year={2024},
      eprint={2211.10438},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.10438}, 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% evaluation  %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@misc{rei2020cometneuralframeworkmt,
      title={COMET: A Neural Framework for MT Evaluation}, 
      author={Ricardo Rei and Craig Stewart and Ana C Farinha and Alon Lavie},
      year={2020},
      eprint={2009.09025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2009.09025}, 
}
