
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% SAAC %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{arasaac,
    author       = {{ARASAAC}},
    title        = {ARASAAC - Portal de Comunicación Aumentativa y Alternativa},
    year         = 2024,
    url          = {https://arasaac.org/aac/es},
    note         = {Accessed: 2024-08-04}
}

@misc{asterics_grid_comunicador,
    author       = {{Aula Abierta ARASAAC}},
    title        = {Asterics Grid: Pantalla de Inicio del Comunicador},
    year         = 2024,
    url          = {https://aulaabierta.arasaac.org/asterics-grid_pantalla-de-inicio-del-comunicador},
    note         = {Accessed: 2024-08-04}
}

@book{murphy2022probabilistic,
  title={Probabilistic Machine Learning: An Introduction},
  author={Murphy, Kevin P.},
  year={2022},
  publisher={The MIT Press},
  address={Cambridge, Massachusetts; London, England}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% APRENDIZAJE AUTOMÁTICO %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@Inbook{samuelCheckers,
  editor = {Sammut, Claude and Webb, Geoffrey I.},
  title = {Samuel's Checkers Player},
  bookTitle = {Encyclopedia of Machine Learning},
  year = {2010},
  publisher = {Springer US},
  address = {Boston, MA},
  pages = {881--881},
  isbn = {978-0-387-30164-8},
  url = {https://doi.org/10.1007/978-0-387-30164-8_740},
}


@book{mitchell1997mcgraw,
  title={Machine Learning},
  author={Mitchell, Tom},
  publisher={McGraw-Hil},
  pages={154--200},
  year={1997}
}
@article{RevModPhys.34.123,
  title = {The Perceptron: A Model for Brain Functioning. I},
  author = {Block, H. D.},
  journal = {Rev. Mod. Phys.},
  volume = {34},
  issue = {1},
  pages = {123--135},
  numpages = {0},
  year = {1962},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.34.123},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.34.123}
}


@book{mirtaheri2022machine,
  title={Machine Learning},
  author={Mirtaheri, Seyedeh Leili and Shahbazian, Reza},
  year={2022},
  publisher={CRC Press},
}

@book{alma997066713303706,
author = {Sowmya V. B. and Majumder, Bodhisattwa and Gupta, Anuj and Surana, Harshit},
address = {Sebastopol, CA},
booktitle = {Practical natural language processing : a comprehensive guide to building real-world NLP systems},
edition = {First edition.},
isbn = {1-4920-5402-X},
keywords = {Application software -- Development},
language = {eng},
publisher = {O'Reilly Media},
title = {Practical natural language processing : a comprehensive guide to building real-world NLP systems },
year = {2020 - 2020},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% REDES NEURONALES %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={Bulletin of Mathematical Biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer},
  doi={10.1007/BF02478259}
}

@book{jurafsky2023speech,
  title={Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author={Jurafsky, Daniel and Martin, James H.},
  edition={Third Edition draft},
  publisher={Stanford University and University of Colorado at Boulder},
  year={2023},
  note={Draft version}
}

@inproceedings{sutskever2014sequencesequencelearningneural,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
title = {Sequence to sequence learning with neural networks},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3104–3112},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}


@Inbook{Rojas1996,
author="Rojas, Ra{\'u}l",
title="The Backpropagation Algorithm",
bookTitle="Neural Networks: A Systematic Introduction",
year="1996",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="149--182",
abstract="We saw in the last chapter that multilayered networks are capable of computing a wider range of Boolean functions than networks with a single layer of computing units. However the computational effort needed for finding the correct combination of weights increases substantially when more parameters and more complicated topologies are considered. In this chapter we discuss a popular learning method capable of handling such large learning problems---the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.",
isbn="978-3-642-61068-4",
doi="10.1007/978-3-642-61068-4_7",
url="https://doi.org/10.1007/978-3-642-61068-4_7"
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% TRANSFORMERS %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{vaswani2023attentionneed,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@INPROCEEDINGS{multiheaddotproduct,
  author={Bilonoh, Bohdan and Mashtalir, Sergii},
  booktitle={2020 IEEE Third International Conference on Data Stream Mining \& Processing (DSMP)}, 
  title={Parallel multi-head dot product attention for video summarization}, 
  year={2020},
  volume={},
  number={},
  pages={158-162},
  keywords={Computational modeling;Computer architecture;Training;YouTube;Natural language processing;Adaptation models;Task analysis;video summarization;sequence to sequence;self-attention;Transformer},
  doi={10.1109/DSMP47368.2020.9204059}}

@misc{cordonnier2021multiheadattentioncollaborateinstead,
      title={Multi-Head Attention: Collaborate Instead of Concatenate}, 
      author={Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},
      year={2021},
      eprint={2006.16362},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16362}, 
}

@misc{ba2016layernormalization,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1607.06450}, 
}


@inproceedings{sriram2017coldfusiontrainingseq2seq,
  title={Cold Fusion: Training Seq2Seq Models Together with Language Models},
  author={Anuroop Sriram and Heewoo Jun and Sanjeev Satheesh and Adam Coates},
  booktitle={Interspeech},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:31004450}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% LANGUAGE MODELS %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{burtsev2023working,
  title={The Working Limitations of Large Language Models},
  author={Burtsev, Mikhail and Reeves, Martin and Job, Adam},
  journal={MIT Sloan Management Review},
  year={2023},
  month={November},
  pages={7},
}

@inproceedings{devlin2019bertpretrainingdeepbidirectional,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@misc{dai2019transformerxlattentivelanguagemodels,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
      year={2019},
      month = {july},
      address = {Florence, Italy},
      url={https://aclanthology.org/P19-1000}, 
     publisher = {Association for Computational Linguistics},
     pages = {2978--2988},
}

@inproceedings{wei2023chainofthoughtpromptingelicitsreasoning,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 year = {2022},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24824--24837},
 publisher = {Curran Associates, Inc.},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
 volume = {35},
series = {NIPS'22},
}

@inproceedings{wang2023selfconsistencyimproveschainthought,
title	= {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
author	= {Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V. Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
year	= {2023},
URL	= {https://arxiv.org/abs/2203.11171},
booktitle	= {ICLR 2023}}

@misc{gu2017trainablegreedydecodingneural,
      title={Trainable Greedy Decoding for Neural Machine Translation}, 
      author={Jiatao Gu and Kyunghyun Cho and Victor O. K. Li},
      year={2017},
      eprint={1702.02429},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1702.02429}, 
}

@inproceedings{nadeem2020systematiccharacterizationsamplingalgorithms,
    title = "A Systematic Characterization of Sampling Algorithms for Open-ended Language Generation",
    author = "Nadeem, Moin  and
      He, Tianxing  and
      Cho, Kyunghyun  and
      Glass, James",
    editor = "Wong, Kam-Fai  and
      Knight, Kevin  and
      Wu, Hua",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-main.36",
    pages = "334--346",
}

@misc{holtzman2020curiouscaseneuraltext,
      title={The Curious Case of Neural Text Degeneration}, 
      author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
      year={2020},
      eprint={1904.09751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09751}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{gemmateam2024gemmaopenmodelsbased,
      title={Gemma: Open Models Based on Gemini Research and Technology}, 
      author={Gemma Team},
      year={2024},
      eprint={2403.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08295}, 
}

@misc{geminiteam2024geminifamilyhighlycapable,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@inproceedings{christiano2023deepreinforcementlearninghuman,
author = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
title = {Deep reinforcement learning from human preferences},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4302–4310},
numpages = {9},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@misc{shazeer2019fasttransformerdecodingwritehead,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1911.02150}, 
}

@article{su2023roformerenhancedtransformerrotary,
title = {RoFormer: Enhanced transformer with Rotary Position Embedding},
journal = {Neurocomputing},
volume = {568},
pages = {127063},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127063},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
author = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu},
keywords = {Pre-trained language models, Position information encoding, Pre-training, Natural language processing},
abstract = {Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer.}
}

@misc{shazeer2020gluvariantsimprovetransformer,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202}, 
}

@inbook{zhang2019rootmeansquarelayer,
author = {Zhang, Biao and Sennrich, Rico},
title = {Root mean square layer normalization},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p\% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7\%~64\% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1110},
numpages = {12}
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@inproceedings{liu2018generatingwikipediasummarizinglong,
  author       = {Peter J. Liu and
                  Mohammad Saleh and
                  Etienne Pot and
                  Ben Goodrich and
                  Ryan Sepassi and
                  Lukasz Kaiser and
                  Noam Shazeer},
  title        = {Generating Wikipedia by Summarizing Long Sequences},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=Hyg0vbWC-},
  timestamp    = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LiuSPGSKS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Adaptación LLM %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@ARTICLE{9134370,
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE}, 
  title={A Comprehensive Survey on Transfer Learning}, 
  year={2021},
  volume={109},
  number={1},
  pages={43-76},
  keywords={Transfer learning;Semisupervised learning;Data models;Covariance matrices;Machine learning;Adaptation models;Domain adaptation;interpretation;machine learning;transfer learning},
  doi={10.1109/JPROC.2020.3004555}}

@article{Weiss2016,
  author    = {Karl Weiss and Taghi M. Khoshgoftaar and DingDing Wang},
  title     = {A survey of transfer learning},
  journal   = {Journal of Big Data},
  year      = {2016},
  volume    = {3},
  number    = {1},
  pages     = {9},
  doi       = {10.1186/s40537-016-0043-6},
  url       = {https://doi.org/10.1186/s40537-016-0043-6},
  issn      = {2196-1115}
}

@INPROCEEDINGS{8659197,
  author={Wang, Tianyang and Huan, Jun and Zhu, Michelle},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Instance-Based Deep Transfer Learning}, 
  year={2019},
  volume={},
  number={},
  pages={367-375},
  keywords={Training;Data models;Training data;Computational modeling;Deep learning;Optimization;Load modeling},
  doi={10.1109/WACV.2019.00045}}

@inproceedings{he2021effectivenessadapterbasedtuningpretrained,
author = {he, Ruidan and Liu, Linlin and Ye, Hai and Qingyu, Tan and Ding, Bosheng and Cheng, Liying and Low, Jiawei and Bing, Lidong and Si, Luo},
year = {2021},
month = {01},
pages = {2208-2222},
title = {On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation},
doi = {10.18653/v1/2021.acl-long.172}
}

@InProceedings{houlsby2019parameterefficienttransferlearningnlp,
  title = 	 {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2790--2799},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/houlsby19a.html},
}

@InProceedings{stickland2019bertpalsprojectedattention,
  title = 	 {{BERT} and {PAL}s: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning},
  author =       {Stickland, Asa Cooper and Murray, Iain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5986--5995},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/stickland19a/stickland19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/stickland19a.html},
}


%peft
@inproceedings{hu2023llmadaptersadapterfamilyparameterefficient,
    title = "{LLM}-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
    author = "Hu, Zhiqiang  and
      Wang, Lei  and
      Lan, Yihuai  and
      Xu, Wanyu  and
      Lim, Ee-Peng  and
      Bing, Lidong  and
      Xu, Xing  and
      Poria, Soujanya  and
      Lee, Roy",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.319",
    doi = "10.18653/v1/2023.emnlp-main.319",
    pages = "5254--5276",
}

%lora
@inproceedings{hu2021loralowrankadaptationlarge,
author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Yuanzhi  Li and Wang, Shean and Wang, Lu and Chen, Weizhu},
title = {LoRA: Low-Rank Adaptation of Large Language Models},
booktitle = {ICLR 2022},
year = {2022},
month = {April},
url = {https://www.microsoft.com/en-us/research/publication/lora-low-rank-adaptation-of-large-language-models/},
}

@inproceedings{choukroun2019low,
  title={Low-bit quantization of neural networks for efficient inference},
  author={Choukroun, Yoni and Kravchik, Eli and Yang, Fan and Kisilev, Pavel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  pages={3009--3018},
  year={2019},
  organization={IEEE}
}

@inproceedings{
micikevicius2018mixedprecisiontraining,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1gs9JgRZ},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% cuantización %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{xiao2024smoothquantaccurateefficientposttraining,
author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
title = {SmoothQuant: accurate and efficient post-training quantization for large language models},
year = {2023},
publisher = {JMLR.org},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1585},
numpages = {13},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% evaluation  %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@online{googleTranslate,
  title     = {Evaluate Custom Models with AutoML Translation},
  author    = {{Google Cloud}},
  year      = {2024},
  url       = {https://cloud.google.com/translate/automl/docs/evaluate},
  note      = {Accessed: 2024-08-10}
}

@inproceedings{rei2020cometneuralframeworkmt,
    title = "{COMET}: A Neural Framework for {MT} Evaluation",
    author = "Rei, Ricardo  and
      Stewart, Craig  and
      Farinha, Ana C  and
      Lavie, Alon",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.213",
    doi = "10.18653/v1/2020.emnlp-main.213",
    pages = "2685--2702",
}
