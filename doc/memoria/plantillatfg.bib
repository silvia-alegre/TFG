
@book{murphy2022probabilistic,
  title={Probabilistic Machine Learning: An Introduction},
  author={Murphy, Kevin P.},
  year={2022},
  publisher={The MIT Press},
  address={Cambridge, Massachusetts; London, England}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% APRENDIZAJE AUTOM√ÅTICO %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@Inbook{samuelCheckers,
  editor = {Sammut, Claude and Webb, Geoffrey I.},
  title = {Samuel's Checkers Player},
  bookTitle = {Encyclopedia of Machine Learning},
  year = {2010},
  publisher = {Springer US},
  address = {Boston, MA},
  pages = {881--881},
  isbn = {978-0-387-30164-8},
  url = {\url{https://doi.org/10.1007/978-0-387-30164-8_740}},
}


@book{mitchell1997mcgraw,
  title={Machine Learningl},
  author={Mitchell, Tom},
  publisher={McGraw-Hil},
  pages={154--200},
  year={1997}
}

@book{mirtaheri2022machine,
  title={Machine Learning},
  author={Mirtaheri, Seyedeh Leili and Shahbazian, Reza},
  year={2022},
  publisher={CRC Press},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% REDES NEURONALES %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={Bulletin of Mathematical Biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer},
  doi={10.1007/BF02478259}
}

@book{jurafsky2023speech,
  title={Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author={Jurafsky, Daniel and Martin, James H.},
  edition={Third Edition draft},
  publisher={Stanford University and University of Colorado at Boulder},
  year={2023},
  note={Draft version}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% TRANSFORMERS %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@INPROCEEDINGS{multiheaddotproduct,
  author={Bilonoh, Bohdan and Mashtalir, Sergii},
  booktitle={2020 IEEE Third International Conference on Data Stream Mining \& Processing (DSMP)}, 
  title={Parallel multi-head dot product attention for video summarization}, 
  year={2020},
  volume={},
  number={},
  pages={158-162},
  keywords={Computational modeling;Computer architecture;Training;YouTube;Natural language processing;Adaptation models;Task analysis;video summarization;sequence to sequence;self-attention;Transformer},
  doi={10.1109/DSMP47368.2020.9204059}}

@misc{cordonnier2021multiheadattentioncollaborateinstead,
      title={Multi-Head Attention: Collaborate Instead of Concatenate}, 
      author={Jean-Baptiste Cordonnier and Andreas Loukas and Martin Jaggi},
      year={2021},
      eprint={2006.16362},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16362}, 
}

@misc{ba2016layernormalization,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1607.06450}, 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% LANGUAGE MODELS %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{dai2019transformerxlattentivelanguagemodels,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.02860}, 
}

%lora
@article{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

